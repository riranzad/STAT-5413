[
["index.html", "Notes for STAT 5413 - Spatial Statistics Preface", " Notes for STAT 5413 - Spatial Statistics John Tipton Fall 2020 Semester. Last Modified: 2020-01-15 Preface These are the lecture notes for STAT 5413 Fall 2020. "],
["day-1.html", "1 Day 1 1.1 Notation 1.2 Probability Distributions 1.3 Hierarchical modeling", " 1 Day 1 library(tidyverse) 1.1 Notation The dimensions of different mathematical objects are very important for the study of spatial statistics. To communicate this, we use the following notation. A scalar random variable is represented by a lowercase alphanumeric letter (\\(x\\), \\(y\\), \\(z\\), etc.), a vector random variable is respresented by a bold lowercase alphanumeric letter (\\(\\mathbf{x}\\), \\(\\mathbf{y}\\), \\(\\mathbf{z}\\), etc.), and a matrix random variable is respresented by a bold uppercase alphanumeric letter (\\(\\mathbf{X}\\), \\(\\mathbf{Y}\\), \\(\\mathbf{Z}\\), etc.). We use a similar notation for parameters as well where scalar parameters are represented by a lowercase Greek letter (\\(\\mu\\), \\(\\alpha\\), \\(\\beta\\), etc.), a vector parameter is respresented by a bold lowercase Greek letter (\\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\alpha}\\), \\(\\boldsymbol{\\beta}\\), etc.), and a matrix random variable is respresented by a bold uppercase Greek letter (\\(\\boldsymbol{\\Sigma}\\), \\(\\boldsymbol{\\Psi}\\), \\(\\boldsymbol{\\Gamma}\\), etc.). 1.2 Probability Distributions We also need notation to explain probability distributions. We use the notation \\([y]\\) to denote the probability density function \\(p(y)\\) of the random variable \\(y\\) and \\([y|x]\\) to denote the probability density function \\(p(y|x)\\) of \\(y\\) given \\(x\\). For example, if \\(y\\) is a Gaussian random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) we write \\[\\begin{align*} [y | \\mu, \\sigma] &amp; = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{-\\frac{1}{2 \\sigma^2} (y - \\mu)^2 \\right\\}. \\end{align*}\\] We can also denote that \\(y\\) has a Gaussian (normal) distribution given mean \\(\\mu\\) and variance \\(\\sigma^2\\) using the \\(\\sim\\) notation \\[\\begin{align*} y | \\mu, \\sigma &amp; \\sim \\operatorname{N}(\\mu, \\sigma^2). \\end{align*}\\] 1.2.1 Example: linear regression \\[\\begin{align*} \\left[y_i | \\boldsymbol{\\theta} \\right] &amp; \\sim \\operatorname{N}(X_i \\beta, \\sigma^2) \\\\ \\boldsymbol{\\theta} &amp; = (\\beta, \\sigma^2) \\end{align*}\\] ## Sample data set.seed(404) dat &lt;- data.frame(x=(x=runif(200, 0, 50)), y=rnorm(200, 10 * x, 100)) ## breaks: where you want to compute densities breaks &lt;- seq(0, max(dat$x), len=7)[-c(1, 7)] dat$section &lt;- cut(dat$x, breaks) ## Get the residuals dat$res &lt;- residuals(lm(y ~ x, data=dat)) ## Compute densities for each section, and flip the axes, and add means of sections ## Note: the densities need to be scaled in relation to the section size (2000 here) ys &lt;- seq(-300, 300, length = 50) xs &lt;- rep(breaks, each = 50) + 1000 * dnorm(ys, 0, 100) res &lt;- matrix(0, 50, 5) for (i in 1:5) { res[, i] &lt;- 10 * breaks[i] + ys } dens &lt;- data.frame(x = xs, y=c(res), grouping = cut(xs, breaks)) ggplot(dat, aes(x, y)) + geom_point(size = 2) + geom_smooth(method=&quot;lm&quot;, fill=NA, lwd=2, se = FALSE) + geom_path(data=dens, aes(x, y, group = grouping), color=&quot;salmon&quot;, lwd=2) + theme_bw() + geom_vline(xintercept=breaks, lty=2) 1.3 Hierarchical modeling Follow Berliner (1996) framework for hierarchical probability models Model encodes our understanding of the scientific process of interest Model accounts for as much uncertainty as possible Model results in a probability distribution Note: nature may be deterministic – often probabilistic models outperform physical models. Example: model individual rain drops vs. probability/intensity of rain Update model with data Use the model to generate parameter estimates given data 1.3.1 Bayesian Hierarchical models (BHMs) Break the model into components: Data Model. Process Model. Parameter Model. Combined, the data model, the process model, and the parameter model define a posterior distribution. \\[\\begin{align*} \\color{cyan}{[\\mathbf{z}, \\boldsymbol{\\theta}_D, \\boldsymbol{\\theta}_P | \\mathbf{y}]} &amp; \\propto \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\color{orange}{[\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P]} \\end{align*}\\] 1.3.2 Empirical Hierarchical models (EHMs) Break the model into components: Data Model. Process Model. Parameter estimates (fixed values) are substituted before fitting the model Combined, the data model and the process model define a predictive distribution. Thus, numerical evaluation of the predictive distribution is typically required to estimate unceratinty (bootstrap, MLE asymptotics) Note: the predictive distribution is not a posterior distribution because the normalizing constant is not known \\[\\begin{align*} \\color{plum}{[\\mathbf{z} | \\mathbf{y}]} &amp; \\propto \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\end{align*}\\] 1.3.3 Data Model \\[\\begin{align*} \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\end{align*}\\] Describes how the data are collected and observed. Account for measurement process and uncertainty. Model the data in the manner in which they were collected. Data \\(\\mathbf{y}\\). Noisy. Expensive. Not what you want to make inference on. Latent variables \\(\\mathbf{z}\\). Think of \\(\\mathbf{z}\\) as the ideal data. No measurement error - the exact quantity you want to observe but can’t. Data model parameters \\(\\boldsymbol{\\theta}_D\\). 1.3.4 Process Model \\[\\begin{align*} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\end{align*}\\] Where the science happens! Latent process \\(\\mathbf{z}\\) is modeled. Can be dynamic in space and/or time Process parameters \\(\\boldsymbol{\\theta}_P\\). Virtually all interesting scientific questions can be made with inference about \\(\\mathbf{z}\\) 1.3.5 Parameter (Prior) Model (BMHs only) \\[\\begin{align*} \\color{orange}{[\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P]} \\end{align*}\\] Probability distributions define “reasonable” ranges for parameters. Parameter models are useful for a variety of problems: Choosing important variables. Preventing over-fitting (regularization). “Pooling” estimates across categories. 1.3.6 Posterior Distribution \\[\\begin{align*} \\color{cyan}{[\\mathbf{z}, \\boldsymbol{\\theta}_D, \\boldsymbol{\\theta}_P | \\mathbf{y}]} &amp; \\propto [\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}] [\\mathbf{z} | \\boldsymbol{\\theta}_P] [\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P] \\end{align*}\\] Probability distribution over all unknowns in the model. Inference is made using the posterior distribution. Because the posterior distribution is a probability distribution (BHMs), uncertainty is easy to calculate. This is not true for EHMs. 1.3.7 Scientifically Motivated Statistical Modeling Criticize the model Does the model fit the data well? Do the predictions make sense? Are there subsets of the data that don’t fit the model well? Make inference using the model. If the model fits the data, use the model fit for prediction or inference. References "],
["day-2.html", "2 Day 2 2.1 Spatial Data 2.2 Types of spatial data", " 2 Day 2 library(tidyverse) library(here) library(sp) library(spatstat) 2.1 Spatial Data All data occur at some location is space and time. For know we focus on spatial analyses and will later extend this to spatio-temporal analyses. Let \\(\\mathcal{D}\\) represent the spatial domain and let \\(\\mathbf{s}\\) be a spatial location. In general, we will let \\(\\mathcal{A} \\subset \\mathcal{D}\\) be a subdomain of the spatial region of \\(\\mathbf{D}\\). Insert Diagram from class here 2.2 Types of spatial data There are three primary types of spatial data that we are going to consider 2.2.1 Geostatistical data Occur everywhere continuous support examples: temperature, precipitation data(&quot;NOAA_df_1990&quot;, package = &quot;STRbook&quot;) glimpse(NOAA_df_1990) ## Observations: 730,486 ## Variables: 10 ## $ julian &lt;int&gt; 726834, 726835, 726836, 726837, 726838, 726839, 726840, 726841… ## $ year &lt;int&gt; 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 19… ## $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ day &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… ## $ id &lt;dbl&gt; 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 38… ## $ z &lt;dbl&gt; 35, 42, 49, 59, 41, 45, 46, 42, 54, 43, 52, 38, 32, 43, 53, 55… ## $ proc &lt;chr&gt; &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;… ## $ lat &lt;dbl&gt; 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35,… ## $ lon &lt;dbl&gt; -81.43333, -81.43333, -81.43333, -81.43333, -81.43333, -81.433… ## $ date &lt;date&gt; 1990-01-01, 1990-01-02, 1990-01-03, 1990-01-04, 1990-01-05, 1… ## Only plot the states with data states &lt;- map_data(&quot;state&quot;) states &lt;- states %&gt;% subset(!(region %in% c(&quot;washington&quot;, &quot;oregon&quot;, &quot;california&quot;, &quot;nevada&quot;, &quot;idaho&quot;, &quot;utah&quot;, &quot;arizona&quot;,&quot;montana&quot;, &quot;wyoming&quot;, &quot;colorado&quot;, &quot;new mexico&quot;))) ## generate map NOAA_df_1990 %&gt;% subset(year == 1990 &amp; day == 1 &amp; proc == &quot;Tmax&quot;) %&gt;% ggplot(aes(x = lon, y = lat, color = z)) + geom_point() + facet_wrap(~ month, scales = &quot;free&quot;, nrow = 4) + geom_polygon(data = states, aes(x = long, y = lat, group = group), inherit.aes = FALSE, fill = NA, color = &quot;black&quot;) + scale_color_viridis_c(option = &quot;inferno&quot;) + ggtitle(&quot;Tmax for the first day of each month in 1990&quot;) 2.2.2 Areal data Occur only over discrete areas can be thought of as an integral of a continuous process over a subdomain \\(\\mathcal{A} \\in \\mathcal{D}\\) examples: cases of a disease by counties, votes in an election by congressional district data(&quot;BEA&quot;, package = &quot;STRbook&quot;) glimpse(BEA) ## Observations: 116 ## Variables: 5 ## $ Description &lt;chr&gt; &quot;Per capita personal income (dollars)&quot;, &quot;Per capita perso… ## $ NAME10 &lt;fct&gt; &quot;Adair, MO&quot;, &quot;Andrew, MO&quot;, &quot;Atchison, MO&quot;, &quot;Audrain, MO&quot;,… ## $ X1970 &lt;int&gt; 2723, 3577, 3770, 3678, 3021, 2832, 3263, 2508, 2147, 349… ## $ X1980 &lt;int&gt; 7399, 7937, 5743, 8356, 7210, 7445, 8596, 6125, 5431, 923… ## $ X1990 &lt;int&gt; 12755, 15059, 14748, 15198, 12873, 13530, 13195, 11854, 1… data(&quot;MOcounties&quot;, package = &quot;STRbook&quot;) glimpse(MOcounties) ## Observations: 214,279 ## Variables: 53 ## $ long &lt;dbl&gt; 627911.9, 627921.4, 627923.0, 627947.8, 627956.5, 627994.8… ## $ lat &lt;dbl&gt; 4473554, 4473559, 4473560, 4473577, 4473583, 4473612, 4473… ## $ order &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ hole &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ piece &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ id &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;… ## $ group &lt;fct&gt; 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1… ## $ STATEFP10 &lt;fct&gt; 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29… ## $ COUNTYFP10 &lt;fct&gt; 045, 045, 045, 045, 045, 045, 045, 045, 045, 045, 045, 045… ## $ COUNTYNS10 &lt;fct&gt; 00758477, 00758477, 00758477, 00758477, 00758477, 00758477… ## $ GEOID10 &lt;fct&gt; 29045, 29045, 29045, 29045, 29045, 29045, 29045, 29045, 29… ## $ NAME10 &lt;fct&gt; &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark… ## $ NAMELSAD10 &lt;fct&gt; Clark County, Clark County, Clark County, Clark County, Cl… ## $ LSAD10 &lt;fct&gt; 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06… ## $ CLASSFP10 &lt;fct&gt; H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1… ## $ MTFCC10 &lt;fct&gt; G4020, G4020, G4020, G4020, G4020, G4020, G4020, G4020, G4… ## $ CSAFP10 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ CBSAFP10 &lt;fct&gt; 22800, 22800, 22800, 22800, 22800, 22800, 22800, 22800, 22… ## $ METDIVFP10 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ FUNCSTAT10 &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A… ## $ ALAND10 &lt;dbl&gt; 1307146971, 1307146971, 1307146971, 1307146971, 1307146971… ## $ AWATER10 &lt;dbl&gt; 18473547, 18473547, 18473547, 18473547, 18473547, 18473547… ## $ INTPTLAT10 &lt;fct&gt; +40.4072748, +40.4072748, +40.4072748, +40.4072748, +40.40… ## $ INTPTLON10 &lt;fct&gt; -091.7294720, -091.7294720, -091.7294720, -091.7294720, -0… ## $ AREA &lt;dbl&gt; 1324937990, 1324937990, 1324937990, 1324937990, 1324937990… ## $ PERIMETER &lt;dbl&gt; 161503.6, 161503.6, 161503.6, 161503.6, 161503.6, 161503.6… ## $ COUNTY10_ &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ COUNTY10_I &lt;int&gt; 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115… ## $ POP90 &lt;int&gt; 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547… ## $ WHITE90 &lt;int&gt; 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528… ## $ BLACK90 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3… ## $ ASIANPI90 &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4… ## $ AMIND90 &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7… ## $ OTHER90 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ HISP90 &lt;int&gt; 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26… ## $ POP00 &lt;int&gt; 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416… ## $ WHITE00 &lt;int&gt; 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329… ## $ BLACK00 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ ASIAN00 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ AMIND00 &lt;int&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15… ## $ HAWNPI00 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ OTHER00 &lt;int&gt; 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16… ## $ MULTRA00 &lt;int&gt; 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45… ## $ HISP00 &lt;int&gt; 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52… ## $ POP10 &lt;int&gt; 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139… ## $ WHITE10 &lt;int&gt; 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011… ## $ BLACK10 &lt;int&gt; 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19… ## $ ASIAN10 &lt;int&gt; 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23… ## $ AMIND10 &lt;int&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9… ## $ HAWNPI10 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ OTHER10 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ MULTRA10 &lt;int&gt; 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72… ## $ HISP10 &lt;int&gt; 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42… MOcounties &lt;- left_join(MOcounties, BEA, by = &quot;NAME10&quot;) ggplot(MOcounties) + geom_polygon(aes(x = long, y = lat, # county boundary group = NAME10, # county group fill = log(X1970))) + # log of income geom_path(aes(x = long, y = lat, group = NAME10)) + scale_fill_viridis_c(limits = c(7.5, 10.2), option = &quot;plasma&quot;, name = &quot;log($)&quot;) + coord_fixed() + ggtitle(&quot;1970&quot;) + xlab(&quot;x (m)&quot;) + ylab(&quot;y (m)&quot;) + theme_bw() 2.2.3 Point process data The count and location of the data are random examples: tornados, lightning strikes # uncomment out this line to download the data # load(url(&quot;http://github.com/mgimond/Spatial/raw/master/Data/ppa.RData&quot;)) # save(starbucks, ma, pop, file = here::here(&quot;data&quot;, &quot;ppa-starbucks.RData&quot;)) load(here::here(&quot;data&quot;, &quot;ppa-starbucks.RData&quot;)) glimpse(starbucks) ## List of 5 ## $ window :List of 4 ## ..$ type : chr &quot;rectangle&quot; ## ..$ xrange: num [1:2] 648032 917741 ## ..$ yrange: num [1:2] 4609785 4748107 ## ..$ units :List of 3 ## .. ..$ singular : chr &quot;unit&quot; ## .. ..$ plural : chr &quot;units&quot; ## .. ..$ multiplier: num 1 ## .. ..- attr(*, &quot;class&quot;)= chr &quot;unitname&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;owin&quot; ## $ n : int 171 ## $ x : num [1:171] 917741 911147 902987 876188 875868 ... ## $ y : num [1:171] 4637151 4628510 4628982 4616741 4616719 ... ## $ markformat: chr &quot;none&quot; ## - attr(*, &quot;class&quot;)= chr &quot;ppp&quot; ## uses spatstat library ## add the massachusetts polygon Window(starbucks) &lt;- ma marks(starbucks) &lt;- NULL ## plot using the plot function from spatstat plot(starbucks) "],
["day-3.html", "3 Day 3 3.1 Anouncements 3.2 Files for spatial data 3.3 Textbook package 3.4 Spatial Visualization", " 3 Day 3 library(tidyverse) library(here) library(sp) 3.1 Anouncements Show gitHub page for site https://github.com/jtipton25/STAT-5413 Show how to download files and data 3.2 Files for spatial data Many different file types for spatial data Typically data are in “flat files” like comma-seperated value (CSV) files read.csv(here(&quot;path&quot;, &quot;to&quot;, &quot;file.csv&quot;)) “shapefiles” which can be read using rgdal or maptools packages library(rgdal) library(maptools) “NetCDF” files cane be read using ncdf4 or RNetCDF library(ncdf4) library(RNetCDF) 3.3 Textbook package To install the data from the textbook, go to https://spacetimewithr.org/ and follow the link to the code. # install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;andrewzm/STRbook&quot;) Note that this package is relatively large because it contains a decent amount of spatial data. library(STRbook) ## ## Attaching package: &#39;STRbook&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## MOcounties 3.4 Spatial Visualization 3.4.1 Spatial visualization using fields Simulate a process with some random locations library(fields) ## Loading required package: spam ## Loading required package: dotCall64 ## Loading required package: grid ## Spam version 2.5-1 (2019-12-12) is loaded. ## Type &#39;help( Spam)&#39; or &#39;demo( spam)&#39; for a short introduction ## and overview of this package. ## Help for individual functions is also obtained by adding the ## suffix &#39;.spam&#39; to the function name, e.g. &#39;help( chol.spam)&#39;. ## ## Attaching package: &#39;spam&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## backsolve, forwardsolve ## Loading required package: maps ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map ## See https://github.com/NCAR/Fields for ## an extensive vignette, other supplements and source code ## longitude and latitude of approximately the center of Arkansas lon_lat_center &lt;- c(-92.33, 35.00) n &lt;- 1000 ## simulate some random locations lon &lt;- runif(n, lon_lat_center[1] - 2, lon_lat_center[1] + 2) lat &lt;- runif(n, lon_lat_center[2] - 2, lon_lat_center[2] + 2) y &lt;- rnorm(n, lat + lon, .1) plot(lon, lat) quilt.plot(lon, lat, y, nx = 30, ny = 30) points(lon, lat, cex = .3) quilt.plot(lon, lat, y, nx = 10, ny = 10) points(lon, lat, cex = .3) Simulate a process on a regular grid n &lt;- 50^2 ## simulate locations on a grid lon &lt;- seq(lon_lat_center[1] - 2, lon_lat_center[1] + 2, length = sqrt(n)) lat &lt;- seq(lon_lat_center[2] - 2, lon_lat_center[2] + 2, length = sqrt(n)) s &lt;- expand.grid(lon, lat) head(lon) ## [1] -94.33000 -94.24837 -94.16673 -94.08510 -94.00347 -93.92184 head(lat) ## [1] 33.00000 33.08163 33.16327 33.24490 33.32653 33.40816 head(s) ## Var1 Var2 ## 1 -94.33000 33 ## 2 -94.24837 33 ## 3 -94.16673 33 ## 4 -94.08510 33 ## 5 -94.00347 33 ## 6 -93.92184 33 plot(s, cex = 0.3) ## simulate some fake data with a north/south trend y &lt;- 120 - 1.5 * s[, 2] + matrix(rnorm(n), sqrt(n), sqrt(n)) image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;) contour(lon, lat, y, main = &quot;Contour plot of simulated data&quot;) image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;) contour(lon, lat, y, main = &quot;Contour plot of simulated data&quot;, add = TRUE, nlevels = 10) ## adding in maps library(maps) maps::map(&quot;world&quot;) maps::map(&quot;state&quot;) maps::map(&quot;county&quot;) maps::map(&quot;county&quot;, &quot;Arkansas&quot;) points(s, cex = 0.3) state &lt;- map.where(&quot;state&quot;, x = s[, 1], y = s[, 2]) head(state) ## [1] &quot;texas&quot; &quot;texas&quot; &quot;texas&quot; &quot;texas&quot; &quot;louisiana&quot; &quot;louisiana&quot; table(state) ## state ## arkansas louisiana mississippi missouri texas ## 1903 34 180 351 32 ## subset only points in arkansas dat &lt;- data.frame( lon = s[, 1], lat = s[, 2], state = state ) maps::map(&quot;county&quot;, &quot;Arkansas&quot;) dat %&gt;% subset(state == &quot;arkansas&quot;) %&gt;% points(cex = 0.3) Plot the simulated data with the county boundaries image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;) maps::map(&quot;county&quot;, add = TRUE, lwd = 2) ## change the aspect ratio image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;, asp = 1.3) maps::map(&quot;county&quot;, add = TRUE, lwd = 2) 3.4.2 Spatial visualization using fields nx &lt;- 100 ny &lt;- 100 library(maps) # for map.where # Corner of the USA corners &lt;- c(-124.733056, -66.947028, 24.520833, 49.384472) # create grid grid &lt;- expand.grid( seq(corners[1], corners[2], length = nx), seq(corners[3], corners[4], length = ny) ) dat &lt;- data.frame( lon = grid[, 1], lat = grid[, 2], inUS = ifelse(is.na(map.where(&quot;usa&quot;, x = grid[, 1], y = grid[, 2])), FALSE, TRUE) ) ## Plot only points in the us dat %&gt;% subset(inUS) %&gt;% ## this selects only the true values ggplot(aes(x = lon, y = lat)) + geom_point(size = 0.6, alpha = 0.5) ## Simulate some data over the grid dat$y &lt;- sin(2 * pi * dat$lon / 10) + cos(2 *pi * dat$lat / 10) + sin(2 * pi * dat$lon / 10) * cos(2 *pi * dat$lat / 10) ## plot each of the responses grouped by latitude dat %&gt;% ggplot(aes(x = lon, y = y, group = lat, color = lat)) + geom_line() ## Function to generate maps map_points &lt;- function (dat, color_low = &quot;white&quot;, color_high = &quot;darkred&quot;, color_na = gray(0.9), zeroiswhite = FALSE, xlim = NULL, ylim = NULL, zlim = NULL, mainTitle = NULL, legendTitle = &quot;&quot;) { library(ggplot2) ## check if the data.fram dat contains the correct variables if (is.null(dat$lon)) { stop(&#39;The data.frame dat must contain a &quot;lon&quot; variable&#39;) } if (is.null(dat$lat)) { stop(&#39;The data.frame dat must contain a &quot;lat&quot; variable&#39;) } if (is.null(dat$y)) { stop(&#39;The data.frame dat must contain a &quot;y&quot; variable&#39;) } # Store the base data of the underlying map states &lt;- map_data(&quot;state&quot;) # Set limits for x, y, z if not specified as parameters if (is.null(xlim)) { xlim &lt;- range(dat$lon, na.rm = TRUE) } if (is.null(ylim)) { ylim &lt;- range(dat$lat, na.rm = TRUE) } if (is.null(zlim)) { zlim &lt;- range(dat$y, na.rm = TRUE) } # Create the plot p &lt;- ggplot(dat, aes(x = lon, y = lat)) + theme_bw() p &lt;- p + theme(plot.title = element_text(size = rel(1.5))) p &lt;- p + geom_point(aes(colour = y)) ## add in the map p &lt;- p + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) ## a 1.3 coordinate ratio is visually appealing p &lt;- p + coord_fixed(ratio = 1.3, xlim = xlim, ylim = ylim) p &lt;- p + labs(title = paste(mainTitle, &quot;\\n&quot;, sep=&quot;&quot;), x = &quot;&quot;, y = &quot;&quot;) if(zeroiswhite){ p &lt;- p + scale_colour_gradient2( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } if(!zeroiswhite){ p &lt;- p + scale_colour_gradient( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } return(p) } ## Let&#39;s make some plots dat %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, mainTitle = &quot;Entire United States&quot; ) ## Subset only the US dat %&gt;% subset(inUS) %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, mainTitle = &quot;Entire United States&quot; ) ## plot only a subset of points dat %&gt;% subset(inUS) %&gt;% ## sample 500 points at random sample_n(500) %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, zeroiswhite = TRUE, mainTitle = &quot;Entire United States&quot; ) ## Truncate the southeastern US dat %&gt;% subset(inUS) %&gt;% ## sample 500 points at random sample_n(500) %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, zeroiswhite = TRUE, xlim = c(-95, -75), ylim = c(25, 37.5), mainTitle = &quot;Southeastern United States&quot;, legendTitle = &quot;Widgets&quot; ) Heatmaps can also be used for plotting. In general, there are two ggplot geoms that are useful for spatial data: geom_tile is good for irregularly spaced data, geom_raster is best for regularly spaced data as it is faster to process. ## Function to generate maps map_heat &lt;- function (dat, color_low = &quot;white&quot;, color_high = &quot;darkred&quot;, color_na = gray(0.9), zeroiswhite = FALSE, xlim = NULL, ylim = NULL, zlim = NULL, mainTitle = NULL, legendTitle = &quot;&quot;, geom = &quot;raster&quot;) { library(ggplot2) ## check if the data.fram dat contains the correct variables if (is.null(dat$lon)) { stop(&#39;The data.frame dat must contain a &quot;lon&quot; variable&#39;) } if (is.null(dat$lat)) { stop(&#39;The data.frame dat must contain a &quot;lat&quot; variable&#39;) } if (is.null(dat$y)) { stop(&#39;The data.frame dat must contain a &quot;y&quot; variable&#39;) } if (!(geom %in% c(&quot;raster&quot;, &quot;tile&quot;))) { stop(&#39;The only options for geom are &quot;raster&quot; or &quot;tile&quot;&#39;) } # Store the base data of the underlying map states &lt;- map_data(&quot;state&quot;) # Set limits for x, y, z if not specified as parameters if (is.null(xlim)) { xlim &lt;- range(dat$lon, na.rm = TRUE) } if (is.null(ylim)) { ylim &lt;- range(dat$lat, na.rm = TRUE) } if (is.null(zlim)) { zlim &lt;- range(dat$y, na.rm = TRUE) } # Create the plot p &lt;- ggplot(dat, aes(x = lon, y = lat)) + theme_bw() p &lt;- p + theme(plot.title = element_text(size = rel(1.5))) if (geom == &quot;raster&quot;) { p &lt;- p + geom_raster(aes(fill = y)) } if (geom == &quot;tile&quot;) { p &lt;- p + geom_tile(aes(fill = y)) } ## add in the map p &lt;- p + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) ## a 1.3 coordinate ratio is visually appealing p &lt;- p + coord_fixed(ratio = 1.3, xlim = xlim, ylim = ylim) p &lt;- p + labs(title = paste(mainTitle, &quot;\\n&quot;, sep=&quot;&quot;), x = &quot;&quot;, y = &quot;&quot;) if(zeroiswhite){ p &lt;- p + scale_colour_gradient2( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } if(!zeroiswhite){ p &lt;- p + scale_colour_gradient( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } return(p) } ## Subset only the US dat %&gt;% subset(inUS) %&gt;% map_heat( color_low = &quot;blue&quot;, color_high = &quot;yellow&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;raster&quot; ) ## Subset only the US dat %&gt;% subset(inUS) %&gt;% map_heat( color_low = &quot;blue&quot;, color_high = &quot;yellow&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;tile&quot; ) ## Subsample the data dat %&gt;% subset(inUS) %&gt;% sample_n(1000) %&gt;% map_heat( color_low = &quot;blue&quot;, color_high = &quot;green&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;raster&quot; ) ## Warning in f(...): Raster pixels are placed at uneven horizontal intervals and ## will be shifted. Consider using geom_tile() instead. ## Warning in f(...): Raster pixels are placed at uneven vertical intervals and ## will be shifted. Consider using geom_tile() instead. ## Subsample the data dat %&gt;% subset(inUS) %&gt;% sample_n(1000) %&gt;% map_heat( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;tile&quot; ) Plotting spatial data using google maps ## longitude and latitude of approximately the center of Arkansas arkansas_center &lt;- c(-92.33, 35.00) library(maps) library(ggplot2) library(ggmap) ## Google&#39;s Terms of Service: https://cloud.google.com/maps-platform/terms/. ## Please cite ggmap if you use it! See citation(&quot;ggmap&quot;) for details. lon &lt;- arkansas_center[1] + seq(-2, 2, length = 10) lat &lt;- arkansas_center[2] + seq(-2, 2, length = 10) s &lt;- expand.grid(lon, lat) head(lon) ## [1] -94.33000 -93.88556 -93.44111 -92.99667 -92.55222 -92.10778 head(lat) ## [1] 33.00000 33.44444 33.88889 34.33333 34.77778 35.22222 str(s) ## &#39;data.frame&#39;: 100 obs. of 2 variables: ## $ Var1: num -94.3 -93.9 -93.4 -93 -92.6 ... ## $ Var2: num 33 33 33 33 33 33 33 33 33 33 ... ## - attr(*, &quot;out.attrs&quot;)=List of 2 ## ..$ dim : int 10 10 ## ..$ dimnames:List of 2 ## .. ..$ Var1: chr &quot;Var1=-94.33000&quot; &quot;Var1=-93.88556&quot; &quot;Var1=-93.44111&quot; &quot;Var1=-92.99667&quot; ... ## .. ..$ Var2: chr &quot;Var2=33.00000&quot; &quot;Var2=33.44444&quot; &quot;Var2=33.88889&quot; &quot;Var2=34.33333&quot; ... plot(s) points(arkansas_center, pch = 19, col = 2) dat &lt;- data.frame(lon = lon, lat = lat) Using Google maps requires registration of a key. See https://www.littlemissdata.com/blog/maps for details. Plotting areal data The example is from https://www4.stat.ncsu.edu/~reich/SpatialStats/code/Guns.pdf taken from https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(15)01026-0/fulltext ## process the guns data # load(here::here(&quot;data&quot;, &quot;guns.RData&quot;)) # names(Y)[1:5] # region &lt;- tolower(names(Y)) # region[1:5] # rate &lt;- 10000*Y/N # numlaws &lt;- rowSums(X) # crime &lt;- data.frame(Y=Y,N=N,rate=rate,X=X,numlaws,region=region) # dat &lt;- data.frame( # deaths_2010 = Y, # population = N, # deaths_per_10000 = Z[, 1], # firearm_quartile = Z[, 2], # unemployment_quartile = Z[, 3], # non_firearm_homocide = Z[, 4], # firearm_export_quartile = Z[, 5], # numlaws = apply(X, 1, sum), # region = region # ) # save(dat, file = here::here(&quot;data&quot;, &quot;guns_processed.RData&quot;)) load(here::here(&quot;data&quot;, &quot;guns_processed.RData&quot;)) ## mutate a death rate dat &lt;- dat %&gt;% mutate(rate = 10000 * deaths_2010 / population) dat %&gt;% ggplot(aes(x = numlaws, y = rate, color = region == &quot;arkansas&quot;)) + geom_point() + scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;)) + xlab(&quot;Number of gun control laws&quot;) + ylab(&quot;Homicide rate (deaths/100K)&quot;) + ggtitle(&quot;Arkansas in red&quot;) + theme(legend.position = &quot;none&quot;) lm(rate ~ numlaws, data = dat) %&gt;% summary() ## ## Call: ## lm(formula = rate ~ numlaws, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46715 -0.16720 -0.02576 0.16171 0.72809 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.344693 0.055145 24.385 &lt; 2e-16 *** ## numlaws -0.045276 0.007302 -6.201 1.24e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2867 on 48 degrees of freedom ## Multiple R-squared: 0.4448, Adjusted R-squared: 0.4332 ## F-statistic: 38.45 on 1 and 48 DF, p-value: 1.236e-07 us &lt;- map_data(&quot;state&quot;) head(us) ## long lat group order region subregion ## 1 -87.46201 30.38968 1 1 alabama &lt;NA&gt; ## 2 -87.48493 30.37249 1 2 alabama &lt;NA&gt; ## 3 -87.52503 30.37249 1 3 alabama &lt;NA&gt; ## 4 -87.53076 30.33239 1 4 alabama &lt;NA&gt; ## 5 -87.57087 30.32665 1 5 alabama &lt;NA&gt; ## 6 -87.58806 30.32665 1 6 alabama &lt;NA&gt; gg &lt;- ggplot() gg &lt;- gg + geom_map(data = us, map = us, aes(x = long, y = lat, map_id = region), fill = &quot;#ffffff&quot;, color = &quot;#ffffff&quot;, size = 0.15) ## Warning: Ignoring unknown aesthetics: x, y gg gg &lt;- gg + geom_map( data = dat, map = us, aes(fill = rate, map_id = region), color = &quot;#ffffff&quot;, size = 0.15 ) gg &lt;- gg + scale_fill_continuous( low = &#39;thistle2&#39;, high = &#39;darkred&#39;, guide= &#39;colorbar&#39;, name = &quot;Deaths/100K&quot; ) gg &lt;- gg + labs(x = NULL, y = NULL, title = &quot;Homicide rates&quot;) gg &lt;- gg + coord_map(&quot;albers&quot;, lat0 = 39, lat1 = 45) gg &lt;- gg + theme(panel.border = element_blank()) gg &lt;- gg + theme(panel.background = element_blank()) gg &lt;- gg + theme(axis.ticks = element_blank()) gg &lt;- gg + theme(axis.text = element_blank()) gg The map looks right according to http://www.deathpenaltyinfo.org/murder-rates-nationally-and-state#MRord 3.4.3 In Class Activity: From Lab 2.1 on the textbook site ## Wikle, C. K., Zammit-Mangion, A., and Cressie, N. (2019), ## Spatio-Temporal Statistics with R, Boca Raton, FL: Chapman &amp; Hall/CRC ## Copyright (c) 2019 Wikle, Zammit-Mangion, Cressie ## ## This program is free software; you can redistribute it and/or ## modify it under the terms of the GNU General Public License ## as published by the Free Software Foundation; either version 2 ## of the License, or (at your option) any later version. ## ## This program is distributed in the hope that it will be useful, ## but WITHOUT ANY WARRANTY; without even the implied warranty of ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the ## GNU General Public License for more details. library(&quot;dplyr&quot;) library(&quot;tidyr&quot;) library(&quot;STRbook&quot;) ## ------------------------------------------------------------------------ locs &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Stationinfo.dat&quot;, package = &quot;STRbook&quot;), col.names = c(&quot;id&quot;, &quot;lat&quot;, &quot;lon&quot;)) Times &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Times_1990.dat&quot;, package = &quot;STRbook&quot;), col.names = c(&quot;julian&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;)) Tmax &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Tmax_1990.dat&quot;, package = &quot;STRbook&quot;)) ## ------------------------------------------------------------------------ names(Tmax) &lt;- locs$id ## ------------------------------------------------------------------------ Tmax &lt;- cbind(Times, Tmax) head(names(Tmax), 10) ## ------------------------------------------------------------------------ Tmax_long &lt;- gather(Tmax, id, z, -julian, -year, -month, -day) head(Tmax_long) ## ------------------------------------------------------------------------ Tmax_long$id &lt;- as.integer(Tmax_long$id) ## ----------------------------------------------------------- nrow(Tmax_long) Tmax_long &lt;- filter(Tmax_long, !(z &lt;= -9998)) nrow(Tmax_long) ## ------------------------------------------------------------------------ Tmax_long &lt;- mutate(Tmax_long, proc = &quot;Tmax&quot;) head(Tmax_long) ## ------------------------------------------------------------------------ data(Tmin_long, package = &quot;STRbook&quot;) data(TDP_long, package = &quot;STRbook&quot;) data(Precip_long, package = &quot;STRbook&quot;) ## ------------------------------------------------------------------------ NOAA_df_1990 &lt;- rbind(Tmax_long, Tmin_long, TDP_long, Precip_long) ## ------------------------------------------------------------------------ summ &lt;- group_by(NOAA_df_1990, year, proc) %&gt;% # groupings summarise(mean_proc = mean(z)) # operation ## ------------------------------------------------------------------------ NOAA_precip &lt;- filter(NOAA_df_1990, proc == &quot;Precip&quot; &amp; month == 6) summ &lt;- group_by(NOAA_precip, year, id) %&gt;% summarise(days_no_precip = sum(z == 0)) head(summ) ## ------------------------------------------------------------------------ median(summ$days_no_precip) ## ------------------------------------------------------------- grps &lt;- group_by(NOAA_precip, year, id) summ &lt;- summarise(grps, days_no_precip = sum(z == 0)) ## ------------------------------------------------------------------------ NOAA_df_sorted &lt;- arrange(NOAA_df_1990, julian, id) ## ------------------------------------------------------------------------ df1 &lt;- select(NOAA_df_1990, julian, z) df2 &lt;- select(NOAA_df_1990, -julian) ## ------------------------------------------------------------------------ NOAA_df_1990 &lt;- left_join(NOAA_df_1990, locs, by = &quot;id&quot;) ## ------------------------------------------------------------------------ Tmax_long_sel &lt;- select(Tmax_long, julian, id, z) Tmax_wide &lt;- spread(Tmax_long_sel, id, z) dim(Tmax_wide) ## ------------------------------------------------------------------------ M &lt;- select(Tmax_wide, -julian) %&gt;% as.matrix() ## ----------------------------------------------------------- library(&quot;sp&quot;) library(&quot;spacetime&quot;) ## ------------------------------------------------------------------------ NOAA_df_1990$date &lt;- with(NOAA_df_1990, paste(year, month, day, sep = &quot;-&quot;)) head(NOAA_df_1990$date, 4) # show first four elements ## ------------------------------------------------------------------------ NOAA_df_1990$date &lt;- as.Date(NOAA_df_1990$date) class(NOAA_df_1990$date) ## ------------------------------------------------------------------------ Tmax_long2 &lt;- filter(NOAA_df_1990, proc == &quot;Tmax&quot;) STObj &lt;- stConstruct(x = Tmax_long2, # data set space = c(&quot;lon&quot;, &quot;lat&quot;), # spatial fields time = &quot;date&quot;) # time field class(STObj) ## ------------------------------------------------------------------------ spat_part &lt;- SpatialPoints(coords = Tmax_long2[, c(&quot;lon&quot;, &quot;lat&quot;)]) temp_part &lt;- Tmax_long2$date STObj2 &lt;- STIDF(sp = spat_part, time = temp_part, data = select(Tmax_long2, -date, -lon, -lat)) class(STObj2) ## ------------------------------------------------------------------------ spat_part &lt;- SpatialPoints(coords = locs[, c(&quot;lon&quot;, &quot;lat&quot;)]) temp_part &lt;- with(Times, paste(year, month, day, sep = &quot;-&quot;)) temp_part &lt;- as.Date(temp_part) ## ------------------------------------------------------------------------ Tmax_long3 &lt;- gather(Tmax, id, z, -julian, -year, -month, -day) ## ------------------------------------------------------------------------ Tmax_long3$id &lt;- as.integer(Tmax_long3$id) Tmax_long3 &lt;- arrange(Tmax_long3,julian,id) ## ------------------------------------------------------------------------ all(unique(Tmax_long3$id) == locs$id) ## ------------------------------------------------------------------------ STObj3 &lt;- STFDF(sp = spat_part, time = temp_part, data = Tmax_long3) class(STObj3) ## ------------------------------------------------------------------------ proj4string(STObj3) &lt;- CRS(&quot;+proj=longlat +ellps=WGS84&quot;) ## ------------------------------------------------------------------------ STObj3$z[STObj3$z == -9999] &lt;- NA "],
["day-4.html", "4 Day 4", " 4 Day 4 # install.packages(&quot;webshot&quot;) # webshot::install_phantomjs() DT::datatable(iris) "],
["day-5.html", "5 Day 5", " 5 Day 5 "],
["day-6.html", "6 Day 6", " 6 Day 6 "],
["day-5-1.html", "7 Day 5", " 7 Day 5 "],
["day-6-1.html", "8 Day 6", " 8 Day 6 "],
["day-4-1.html", "9 Day 4", " 9 Day 4 "],
["day-10.html", "10 Day 10", " 10 Day 10 "],
["day-11.html", "11 Day 11", " 11 Day 11 "],
["day-12.html", "12 Day 12", " 12 Day 12 "],
["day-13.html", "13 Day 13", " 13 Day 13 "],
["day-14.html", "14 Day 14", " 14 Day 14 "],
["day-15.html", "15 Day 15", " 15 Day 15 "],
["day-16.html", "16 Day 16", " 16 Day 16 "],
["day-17.html", "17 Day 17", " 17 Day 17 "],
["day-18.html", "18 Day 18", " 18 Day 18 "],
["day-19.html", "19 Day 19", " 19 Day 19 "],
["day-20.html", "20 Day 20", " 20 Day 20 "],
["day-21.html", "21 Day 21", " 21 Day 21 "],
["day-22.html", "22 Day 22", " 22 Day 22 "],
["day-23.html", "23 Day 23", " 23 Day 23 "],
["day-24.html", "24 Day 24", " 24 Day 24 "],
["day-25.html", "25 Day 25", " 25 Day 25 "],
["day-26.html", "26 Day 26", " 26 Day 26 "],
["day-27.html", "27 Day 27", " 27 Day 27 "],
["day-28.html", "28 Day 28", " 28 Day 28 "],
["day-29.html", "29 Day 29", " 29 Day 29 "],
["day-30.html", "30 Day 30", " 30 Day 30 "],
["day-31.html", "31 Day 31", " 31 Day 31 "],
["day-32.html", "32 Day 32", " 32 Day 32 "],
["day-33.html", "33 Day 33", " 33 Day 33 "],
["day-34.html", "34 Day 34", " 34 Day 34 "],
["day-35.html", "35 Day 35", " 35 Day 35 "],
["day-36.html", "36 Day 36", " 36 Day 36 "],
["day-37.html", "37 Day 37", " 37 Day 37 "],
["day-38.html", "38 Day 38", " 38 Day 38 "],
["day-39.html", "39 Day 39", " 39 Day 39 "],
["day-40.html", "40 Day 40", " 40 Day 40 "],
["day-41.html", "41 Day 41", " 41 Day 41 "],
["day-42.html", "42 Day 42", " 42 Day 42 "],
["day-43.html", "43 Day 43", " 43 Day 43 "],
["references.html", "References", " References "]
]
