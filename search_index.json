[
["index.html", "Notes for STAT 5413 - Spatial Statistics Preface", " Notes for STAT 5413 - Spatial Statistics John Tipton Fall 2020 Semester. Last Modified: 2020-01-14 Preface These are the lecture notes for STAT 5413 Fall 2020. "],
["day-1.html", "1 Day 1 1.1 Notation 1.2 Probability Distributions 1.3 Hierarchical modeling", " 1 Day 1 library(tidyverse) 1.1 Notation The dimensions of different mathematical objects are very important for the study of spatial statistics. To communicate this, we use the following notation. A scalar random variable is represented by a lowercase alphanumeric letter (\\(x\\), \\(y\\), \\(z\\), etc.), a vector random variable is respresented by a bold lowercase alphanumeric letter (\\(\\mathbf{x}\\), \\(\\mathbf{y}\\), \\(\\mathbf{z}\\), etc.), and a matrix random variable is respresented by a bold uppercase alphanumeric letter (\\(\\mathbf{X}\\), \\(\\mathbf{Y}\\), \\(\\mathbf{Z}\\), etc.). We use a similar notation for parameters as well where scalar parameters are represented by a lowercase Greek letter (\\(\\mu\\), \\(\\alpha\\), \\(\\beta\\), etc.), a vector parameter is respresented by a bold lowercase Greek letter (\\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\alpha}\\), \\(\\boldsymbol{\\beta}\\), etc.), and a matrix random variable is respresented by a bold uppercase Greek letter (\\(\\boldsymbol{\\Sigma}\\), \\(\\boldsymbol{\\Psi}\\), \\(\\boldsymbol{\\Gamma}\\), etc.). 1.2 Probability Distributions We also need notation to explain probability distributions. We use the notation \\([y]\\) to denote the probability density function \\(p(y)\\) of the random variable \\(y\\) and \\([y|x]\\) to denote the probability density function \\(p(y|x)\\) of \\(y\\) given \\(x\\). For example, if \\(y\\) is a Gaussian random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) we write \\[\\begin{align*} [y | \\mu, \\sigma] &amp; = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{-\\frac{1}{2 \\sigma^2} (y - \\mu)^2 \\right\\}. \\end{align*}\\] We can also denote that \\(y\\) has a Gaussian (normal) distribution given mean \\(\\mu\\) and variance \\(\\sigma^2\\) using the \\(\\sim\\) notation \\[\\begin{align*} y | \\mu, \\sigma &amp; \\sim \\operatorname{N}(\\mu, \\sigma^2). \\end{align*}\\] 1.2.1 Example: linear regression \\[\\begin{align*} \\left[y_i | \\boldsymbol{\\theta} \\right] &amp; \\sim \\operatorname{N}(X_i \\beta, \\sigma^2) \\\\ \\boldsymbol{\\theta} &amp; = (\\beta, \\sigma^2) \\end{align*}\\] ## Sample data set.seed(404) dat &lt;- data.frame(x=(x=runif(200, 0, 50)), y=rnorm(200, 10 * x, 100)) ## breaks: where you want to compute densities breaks &lt;- seq(0, max(dat$x), len=7)[-c(1, 7)] dat$section &lt;- cut(dat$x, breaks) ## Get the residuals dat$res &lt;- residuals(lm(y ~ x, data=dat)) ## Compute densities for each section, and flip the axes, and add means of sections ## Note: the densities need to be scaled in relation to the section size (2000 here) ys &lt;- seq(-300, 300, length = 50) xs &lt;- rep(breaks, each = 50) + 1000 * dnorm(ys, 0, 100) res &lt;- matrix(0, 50, 5) for (i in 1:5) { res[, i] &lt;- 10 * breaks[i] + ys } dens &lt;- data.frame(x = xs, y=c(res), grouping = cut(xs, breaks)) ggplot(dat, aes(x, y)) + geom_point(size = 2) + geom_smooth(method=&quot;lm&quot;, fill=NA, lwd=2, se = FALSE) + geom_path(data=dens, aes(x, y, group = grouping), color=&quot;salmon&quot;, lwd=2) + theme_bw() + geom_vline(xintercept=breaks, lty=2) 1.3 Hierarchical modeling Follow Berliner (1996) framework for hierarchical probability models Model encodes our understanding of the scientific process of interest Model accounts for as much uncertainty as possible Model results in a probability distribution Note: nature may be deterministic – often probabilistic models outperform physical models. Example: model individual rain drops vs. probability/intensity of rain Update model with data Use the model to generate parameter estimates given data 1.3.1 Bayesian Hierarchical models (BHMs) Break the model into components: Data Model. Process Model. Parameter Model. Combined, the data model, the process model, and the parameter model define a posterior distribution. \\[\\begin{align*} \\color{cyan}{[\\mathbf{z}, \\boldsymbol{\\theta}_D, \\boldsymbol{\\theta}_P | \\mathbf{y}]} &amp; \\propto \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\color{orange}{[\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P]} \\end{align*}\\] 1.3.2 Empirical Hierarchical models (EHMs) Break the model into components: Data Model. Process Model. Parameter estimates (fixed values) are substituted before fitting the model Combined, the data model and the process model define a predictive distribution. Thus, numerical evaluation of the predictive distribution is typically required to estimate unceratinty (bootstrap, MLE asymptotics) Note: the predictive distribution is not a posterior distribution because the normalizing constant is not known \\[\\begin{align*} \\color{plum}{[\\mathbf{z} | \\mathbf{y}]} &amp; \\propto \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\end{align*}\\] 1.3.3 Data Model \\[\\begin{align*} \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\end{align*}\\] Describes how the data are collected and observed. Account for measurement process and uncertainty. Model the data in the manner in which they were collected. Data \\(\\mathbf{y}\\). Noisy. Expensive. Not what you want to make inference on. Latent variables \\(\\mathbf{z}\\). Think of \\(\\mathbf{z}\\) as the ideal data. No measurement error - the exact quantity you want to observe but can’t. Data model parameters \\(\\boldsymbol{\\theta}_D\\). 1.3.4 Process Model \\[\\begin{align*} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\end{align*}\\] Where the science happens! Latent process \\(\\mathbf{z}\\) is modeled. Can be dynamic in space and/or time Process parameters \\(\\boldsymbol{\\theta}_P\\). Virtually all interesting scientific questions can be made with inference about \\(\\mathbf{z}\\) 1.3.5 Parameter (Prior) Model (BMHs only) \\[\\begin{align*} \\color{orange}{[\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P]} \\end{align*}\\] Probability distributions define “reasonable” ranges for parameters. Parameter models are useful for a variety of problems: Choosing important variables. Preventing over-fitting (regularization). “Pooling” estimates across categories. 1.3.6 Posterior Distribution \\[\\begin{align*} \\color{cyan}{[\\mathbf{z}, \\boldsymbol{\\theta}_D, \\boldsymbol{\\theta}_P | \\mathbf{y}]} &amp; \\propto [\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}] [\\mathbf{z} | \\boldsymbol{\\theta}_P] [\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P] \\end{align*}\\] Probability distribution over all unknowns in the model. Inference is made using the posterior distribution. Because the posterior distribution is a probability distribution (BHMs), uncertainty is easy to calculate. This is not true for EHMs. 1.3.7 Scientifically Motivated Statistical Modeling Criticize the model Does the model fit the data well? Do the predictions make sense? Are there subsets of the data that don’t fit the model well? Make inference using the model. If the model fits the data, use the model fit for prediction or inference. References "],
["day-2.html", "2 Day 2 2.1 Spatial Data 2.2 Types of spatial data 2.3 Textbook package 2.4 Spatial Visualization", " 2 Day 2 library(tidyverse) library(here) library(sp) 2.1 Spatial Data All data occur at some location is space and time. For know we focus on spatial analyses and will later extend this to spatio-temporal analyses. Let \\(\\mathcal{D}\\) represent the spatial domain and let \\(\\mathbf{s}\\) be a spatial location. In general, we will let \\(\\mathcal{A} \\subset \\mathcal{D}\\) be a subdomain of the spatial region of \\(\\mathbf{D}\\). Insert Diagram from class here 2.2 Types of spatial data There are three primary types of spatial data that we are going to consider 2.2.1 Geostatistical data Occur everywhere continuous support examples: temperature, precipitation data(&quot;NOAA_df_1990&quot;, package = &quot;STRbook&quot;) glimpse(NOAA_df_1990) ## Observations: 730,486 ## Variables: 10 ## $ julian &lt;int&gt; 726834, 726835, 726836, 726837, 726838, 726839, 726840, 726841… ## $ year &lt;int&gt; 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 19… ## $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ day &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… ## $ id &lt;dbl&gt; 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 38… ## $ z &lt;dbl&gt; 35, 42, 49, 59, 41, 45, 46, 42, 54, 43, 52, 38, 32, 43, 53, 55… ## $ proc &lt;chr&gt; &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;… ## $ lat &lt;dbl&gt; 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35,… ## $ lon &lt;dbl&gt; -81.43333, -81.43333, -81.43333, -81.43333, -81.43333, -81.433… ## $ date &lt;date&gt; 1990-01-01, 1990-01-02, 1990-01-03, 1990-01-04, 1990-01-05, 1… ## Only plot the states with data states &lt;- map_data(&quot;state&quot;) states &lt;- states %&gt;% subset(!(region %in% c(&quot;washington&quot;, &quot;oregon&quot;, &quot;california&quot;, &quot;nevada&quot;, &quot;idaho&quot;, &quot;utah&quot;, &quot;arizona&quot;,&quot;montana&quot;, &quot;wyoming&quot;, &quot;colorado&quot;, &quot;new mexico&quot;))) ## generate map NOAA_df_1990 %&gt;% subset(year == 1990 &amp; day == 1 &amp; proc == &quot;Tmax&quot;) %&gt;% ggplot(aes(x = lon, y = lat, color = z)) + geom_point() + facet_wrap(~ month, scales = &quot;free&quot;, nrow = 4) + geom_polygon(data = states, aes(x = long, y = lat, group = group), inherit.aes = FALSE, fill = NA, color = &quot;black&quot;) + scale_color_viridis_c(option = &quot;inferno&quot;) + ggtitle(&quot;Tmax for the first day of each month in 1990&quot;) 2.2.2 Areal data Occur only over discrete areas can be thought of as an integral of a continuous process over a subdomain \\(\\mathcal{A} \\in \\mathcal{D}\\) examples: cases of a disease by counties, votes in an election by congressional district data(&quot;BEA&quot;, package = &quot;STRbook&quot;) glimpse(BEA) ## Observations: 116 ## Variables: 5 ## $ Description &lt;chr&gt; &quot;Per capita personal income (dollars)&quot;, &quot;Per capita perso… ## $ NAME10 &lt;fct&gt; &quot;Adair, MO&quot;, &quot;Andrew, MO&quot;, &quot;Atchison, MO&quot;, &quot;Audrain, MO&quot;,… ## $ X1970 &lt;int&gt; 2723, 3577, 3770, 3678, 3021, 2832, 3263, 2508, 2147, 349… ## $ X1980 &lt;int&gt; 7399, 7937, 5743, 8356, 7210, 7445, 8596, 6125, 5431, 923… ## $ X1990 &lt;int&gt; 12755, 15059, 14748, 15198, 12873, 13530, 13195, 11854, 1… data(&quot;MOcounties&quot;, package = &quot;STRbook&quot;) glimpse(MOcounties) ## Observations: 214,279 ## Variables: 53 ## $ long &lt;dbl&gt; 627911.9, 627921.4, 627923.0, 627947.8, 627956.5, 627994.8… ## $ lat &lt;dbl&gt; 4473554, 4473559, 4473560, 4473577, 4473583, 4473612, 4473… ## $ order &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ hole &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ piece &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ id &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;… ## $ group &lt;fct&gt; 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1… ## $ STATEFP10 &lt;fct&gt; 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29… ## $ COUNTYFP10 &lt;fct&gt; 045, 045, 045, 045, 045, 045, 045, 045, 045, 045, 045, 045… ## $ COUNTYNS10 &lt;fct&gt; 00758477, 00758477, 00758477, 00758477, 00758477, 00758477… ## $ GEOID10 &lt;fct&gt; 29045, 29045, 29045, 29045, 29045, 29045, 29045, 29045, 29… ## $ NAME10 &lt;fct&gt; &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark… ## $ NAMELSAD10 &lt;fct&gt; Clark County, Clark County, Clark County, Clark County, Cl… ## $ LSAD10 &lt;fct&gt; 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06… ## $ CLASSFP10 &lt;fct&gt; H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1… ## $ MTFCC10 &lt;fct&gt; G4020, G4020, G4020, G4020, G4020, G4020, G4020, G4020, G4… ## $ CSAFP10 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ CBSAFP10 &lt;fct&gt; 22800, 22800, 22800, 22800, 22800, 22800, 22800, 22800, 22… ## $ METDIVFP10 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ FUNCSTAT10 &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A… ## $ ALAND10 &lt;dbl&gt; 1307146971, 1307146971, 1307146971, 1307146971, 1307146971… ## $ AWATER10 &lt;dbl&gt; 18473547, 18473547, 18473547, 18473547, 18473547, 18473547… ## $ INTPTLAT10 &lt;fct&gt; +40.4072748, +40.4072748, +40.4072748, +40.4072748, +40.40… ## $ INTPTLON10 &lt;fct&gt; -091.7294720, -091.7294720, -091.7294720, -091.7294720, -0… ## $ AREA &lt;dbl&gt; 1324937990, 1324937990, 1324937990, 1324937990, 1324937990… ## $ PERIMETER &lt;dbl&gt; 161503.6, 161503.6, 161503.6, 161503.6, 161503.6, 161503.6… ## $ COUNTY10_ &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ COUNTY10_I &lt;int&gt; 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115… ## $ POP90 &lt;int&gt; 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547… ## $ WHITE90 &lt;int&gt; 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528… ## $ BLACK90 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3… ## $ ASIANPI90 &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4… ## $ AMIND90 &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7… ## $ OTHER90 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ HISP90 &lt;int&gt; 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26… ## $ POP00 &lt;int&gt; 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416… ## $ WHITE00 &lt;int&gt; 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329… ## $ BLACK00 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ ASIAN00 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ AMIND00 &lt;int&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15… ## $ HAWNPI00 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ OTHER00 &lt;int&gt; 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16… ## $ MULTRA00 &lt;int&gt; 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45… ## $ HISP00 &lt;int&gt; 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52… ## $ POP10 &lt;int&gt; 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139… ## $ WHITE10 &lt;int&gt; 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011… ## $ BLACK10 &lt;int&gt; 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19… ## $ ASIAN10 &lt;int&gt; 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23… ## $ AMIND10 &lt;int&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9… ## $ HAWNPI10 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ OTHER10 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ MULTRA10 &lt;int&gt; 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72… ## $ HISP10 &lt;int&gt; 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42… MOcounties &lt;- left_join(MOcounties, BEA, by = &quot;NAME10&quot;) ggplot(MOcounties) + geom_polygon(aes(x = long, y = lat, # county boundary group = NAME10, # county group fill = log(X1970))) + # log of income geom_path(aes(x = long, y = lat, group = NAME10)) + scale_fill_viridis_c(limits = c(7.5, 10.2), option = &quot;plasma&quot;, name = &quot;log($)&quot;) + coord_fixed() + ggtitle(&quot;1970&quot;) + # county boundary # county group # annotations xlab(&quot;x (m)&quot;) + ylab(&quot;y (m)&quot;) + theme_bw() 2.2.3 Point process data The count and location of the data are random examples: tornados, lightning strikes # uncomment out this line to download the data # load(url(&quot;http://github.com/mgimond/Spatial/raw/master/Data/ppa.RData&quot;)) # save(starbucks, ma, pop, file = here::here(&quot;data&quot;, &quot;ppa-starbucks.RData&quot;)) load(here::here(&quot;data&quot;, &quot;ppa-starbucks.RData&quot;)) glimpse(starbucks) ## List of 5 ## $ window :List of 4 ## ..$ type : chr &quot;rectangle&quot; ## ..$ xrange: num [1:2] 648032 917741 ## ..$ yrange: num [1:2] 4609785 4748107 ## ..$ units :List of 3 ## .. ..$ singular : chr &quot;unit&quot; ## .. ..$ plural : chr &quot;units&quot; ## .. ..$ multiplier: num 1 ## .. ..- attr(*, &quot;class&quot;)= chr &quot;unitname&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;owin&quot; ## $ n : int 171 ## $ x : num [1:171] 917741 911147 902987 876188 875868 ... ## $ y : num [1:171] 4637151 4628510 4628982 4616741 4616719 ... ## $ markformat: chr &quot;none&quot; ## - attr(*, &quot;class&quot;)= chr &quot;ppp&quot; library(spatstat) ## Loading required package: spatstat.data ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse ## Loading required package: rpart ## Registered S3 method overwritten by &#39;spatstat&#39;: ## method from ## print.boxx cli ## ## spatstat 1.62-2 (nickname: &#39;Shape-shifting lizard&#39;) ## For an introduction to spatstat, type &#39;beginner&#39; ## add the massachusetts polygon Window(starbucks) &lt;- ma marks(starbucks) &lt;- NULL ## plot using the plot function from spatstat plot(starbucks) Many different file types for spatial data Typically data are in “flat files” like comma-seperated value (CSV) files read.csv(here(&quot;path&quot;, &quot;to&quot;, &quot;file.csv&quot;)) “shapefiles” which can be read using rgdal or maptools packages library(rgdal) library(maptools) “NetCDF” files cane be read using ncdf4 or RNetCDF library(ncdf4) library(RNetCDF) 2.3 Textbook package To install the data from the textbook, go to https://spacetimewithr.org/ and follow the link to the code. # install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;andrewzm/STRbook&quot;) Note that this package is relatively large because it contains a decent amount of spatial data. library(STRbook) ## ## Attaching package: &#39;STRbook&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## MOcounties 2.4 Spatial Visualization 2.4.1 In Class Activity: From Lab 2.1 on the textbook site ## Wikle, C. K., Zammit-Mangion, A., and Cressie, N. (2019), ## Spatio-Temporal Statistics with R, Boca Raton, FL: Chapman &amp; Hall/CRC ## Copyright (c) 2019 Wikle, Zammit-Mangion, Cressie ## ## This program is free software; you can redistribute it and/or ## modify it under the terms of the GNU General Public License ## as published by the Free Software Foundation; either version 2 ## of the License, or (at your option) any later version. ## ## This program is distributed in the hope that it will be useful, ## but WITHOUT ANY WARRANTY; without even the implied warranty of ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the ## GNU General Public License for more details. library(&quot;dplyr&quot;) library(&quot;tidyr&quot;) library(&quot;STRbook&quot;) ## ------------------------------------------------------------------------ locs &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Stationinfo.dat&quot;, package = &quot;STRbook&quot;), col.names = c(&quot;id&quot;, &quot;lat&quot;, &quot;lon&quot;)) Times &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Times_1990.dat&quot;, package = &quot;STRbook&quot;), col.names = c(&quot;julian&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;)) Tmax &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Tmax_1990.dat&quot;, package = &quot;STRbook&quot;)) ## ------------------------------------------------------------------------ names(Tmax) &lt;- locs$id ## ------------------------------------------------------------------------ Tmax &lt;- cbind(Times, Tmax) head(names(Tmax), 10) ## ------------------------------------------------------------------------ Tmax_long &lt;- gather(Tmax, id, z, -julian, -year, -month, -day) head(Tmax_long) ## ------------------------------------------------------------------------ Tmax_long$id &lt;- as.integer(Tmax_long$id) ## ----------------------------------------------------------- nrow(Tmax_long) Tmax_long &lt;- filter(Tmax_long, !(z &lt;= -9998)) nrow(Tmax_long) ## ------------------------------------------------------------------------ Tmax_long &lt;- mutate(Tmax_long, proc = &quot;Tmax&quot;) head(Tmax_long) ## ------------------------------------------------------------------------ data(Tmin_long, package = &quot;STRbook&quot;) data(TDP_long, package = &quot;STRbook&quot;) data(Precip_long, package = &quot;STRbook&quot;) ## ------------------------------------------------------------------------ NOAA_df_1990 &lt;- rbind(Tmax_long, Tmin_long, TDP_long, Precip_long) ## ------------------------------------------------------------------------ summ &lt;- group_by(NOAA_df_1990, year, proc) %&gt;% # groupings summarise(mean_proc = mean(z)) # operation ## ------------------------------------------------------------------------ NOAA_precip &lt;- filter(NOAA_df_1990, proc == &quot;Precip&quot; &amp; month == 6) summ &lt;- group_by(NOAA_precip, year, id) %&gt;% summarise(days_no_precip = sum(z == 0)) head(summ) ## ------------------------------------------------------------------------ median(summ$days_no_precip) ## ------------------------------------------------------------- grps &lt;- group_by(NOAA_precip, year, id) summ &lt;- summarise(grps, days_no_precip = sum(z == 0)) ## ------------------------------------------------------------------------ NOAA_df_sorted &lt;- arrange(NOAA_df_1990, julian, id) ## ------------------------------------------------------------------------ df1 &lt;- select(NOAA_df_1990, julian, z) df2 &lt;- select(NOAA_df_1990, -julian) ## ------------------------------------------------------------------------ NOAA_df_1990 &lt;- left_join(NOAA_df_1990, locs, by = &quot;id&quot;) ## ------------------------------------------------------------------------ Tmax_long_sel &lt;- select(Tmax_long, julian, id, z) Tmax_wide &lt;- spread(Tmax_long_sel, id, z) dim(Tmax_wide) ## ------------------------------------------------------------------------ M &lt;- select(Tmax_wide, -julian) %&gt;% as.matrix() ## ----------------------------------------------------------- library(&quot;sp&quot;) library(&quot;spacetime&quot;) ## ------------------------------------------------------------------------ NOAA_df_1990$date &lt;- with(NOAA_df_1990, paste(year, month, day, sep = &quot;-&quot;)) head(NOAA_df_1990$date, 4) # show first four elements ## ------------------------------------------------------------------------ NOAA_df_1990$date &lt;- as.Date(NOAA_df_1990$date) class(NOAA_df_1990$date) ## ------------------------------------------------------------------------ Tmax_long2 &lt;- filter(NOAA_df_1990, proc == &quot;Tmax&quot;) STObj &lt;- stConstruct(x = Tmax_long2, # data set space = c(&quot;lon&quot;, &quot;lat&quot;), # spatial fields time = &quot;date&quot;) # time field class(STObj) ## ------------------------------------------------------------------------ spat_part &lt;- SpatialPoints(coords = Tmax_long2[, c(&quot;lon&quot;, &quot;lat&quot;)]) temp_part &lt;- Tmax_long2$date STObj2 &lt;- STIDF(sp = spat_part, time = temp_part, data = select(Tmax_long2, -date, -lon, -lat)) class(STObj2) ## ------------------------------------------------------------------------ spat_part &lt;- SpatialPoints(coords = locs[, c(&quot;lon&quot;, &quot;lat&quot;)]) temp_part &lt;- with(Times, paste(year, month, day, sep = &quot;-&quot;)) temp_part &lt;- as.Date(temp_part) ## ------------------------------------------------------------------------ Tmax_long3 &lt;- gather(Tmax, id, z, -julian, -year, -month, -day) ## ------------------------------------------------------------------------ Tmax_long3$id &lt;- as.integer(Tmax_long3$id) Tmax_long3 &lt;- arrange(Tmax_long3,julian,id) ## ------------------------------------------------------------------------ all(unique(Tmax_long3$id) == locs$id) ## ------------------------------------------------------------------------ STObj3 &lt;- STFDF(sp = spat_part, time = temp_part, data = Tmax_long3) class(STObj3) ## ------------------------------------------------------------------------ proj4string(STObj3) &lt;- CRS(&quot;+proj=longlat +ellps=WGS84&quot;) ## ------------------------------------------------------------------------ STObj3$z[STObj3$z == -9999] &lt;- NA "],
["day-3.html", "3 Day 3", " 3 Day 3 "],
["day-4.html", "4 Day 4", " 4 Day 4 "],
["day-5.html", "5 Day 5", " 5 Day 5 "],
["day-6.html", "6 Day 6", " 6 Day 6 "],
["day-5-1.html", "7 Day 5", " 7 Day 5 "],
["day-6-1.html", "8 Day 6", " 8 Day 6 "],
["day-4-1.html", "9 Day 4", " 9 Day 4 "],
["day-10.html", "10 Day 10", " 10 Day 10 "],
["day-11.html", "11 Day 11", " 11 Day 11 "],
["day-12.html", "12 Day 12", " 12 Day 12 "],
["day-13.html", "13 Day 13", " 13 Day 13 "],
["day-14.html", "14 Day 14", " 14 Day 14 "],
["day-15.html", "15 Day 15", " 15 Day 15 "],
["day-16.html", "16 Day 16", " 16 Day 16 "],
["day-17.html", "17 Day 17", " 17 Day 17 "],
["day-18.html", "18 Day 18", " 18 Day 18 "],
["day-19.html", "19 Day 19", " 19 Day 19 "],
["day-20.html", "20 Day 20", " 20 Day 20 "],
["day-21.html", "21 Day 21", " 21 Day 21 "],
["day-22.html", "22 Day 22", " 22 Day 22 "],
["day-23.html", "23 Day 23", " 23 Day 23 "],
["day-24.html", "24 Day 24", " 24 Day 24 "],
["day-25.html", "25 Day 25", " 25 Day 25 "],
["day-26.html", "26 Day 26", " 26 Day 26 "],
["day-27.html", "27 Day 27", " 27 Day 27 "],
["day-28.html", "28 Day 28", " 28 Day 28 "],
["day-29.html", "29 Day 29", " 29 Day 29 "],
["day-30.html", "30 Day 30", " 30 Day 30 "],
["day-31.html", "31 Day 31", " 31 Day 31 "],
["day-32.html", "32 Day 32", " 32 Day 32 "],
["day-33.html", "33 Day 33", " 33 Day 33 "],
["day-34.html", "34 Day 34", " 34 Day 34 "],
["day-35.html", "35 Day 35", " 35 Day 35 "],
["day-36.html", "36 Day 36", " 36 Day 36 "],
["day-37.html", "37 Day 37", " 37 Day 37 "],
["day-38.html", "38 Day 38", " 38 Day 38 "],
["day-39.html", "39 Day 39", " 39 Day 39 "],
["day-40.html", "40 Day 40", " 40 Day 40 "],
["day-41.html", "41 Day 41", " 41 Day 41 "],
["day-42.html", "42 Day 42", " 42 Day 42 "],
["day-43.html", "43 Day 43", " 43 Day 43 "],
["references.html", "References", " References "]
]
