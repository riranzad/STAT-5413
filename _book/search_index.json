[
["day-14.html", "14 Day 14 14.1 Announcements 14.2 The Metropolis-Hastings Algorithm 14.3 The Metropolis-Hastings algorithm 14.4 Example 14.5 Posterior 14.6 Conditional posterior for \\(\\phi\\) 14.7 Conditional posterior for \\(\\sigma^2\\) 14.8 MCMC using a symmetric proposal (Metropolis)", " 14 Day 14 14.1 Announcements 14.2 The Metropolis-Hastings Algorithm Last time, we discussed the Metropolis algorithm for sampling from a distribution where the conditional posterior distribution is not known in an analytic form. The Metropolis algorithm assumes the proposal distribution is symmetric. The Metropolis-Hastings algorithm allows for asymmetric proposal distributions. A good intuition for how the Metropolis Hastings algorithm works is presented by Chib and Greenberg in Understanding the Metropolis-Hastings Algorithm. This begs the question: Why would one want to use an asymmetric proposal distribution? There are many reasons, but the most compelling and most common is to guarantee that the proposed value of the parameter is in the support of the parameter space. For example, a variance parameter must be positive and a symmetric normal proposal could generate negative proposals that must be rejected with probability 1. This can result in inefficient MCMC samplers that have very low acceptance rates and therefor exhibit poor mixing. Instead, we can use an asymmetric proposal that guarantees a positive proposal and should lead to higer acceptance rates. 14.3 The Metropolis-Hastings algorithm Recall for a symmetric proposal, the Metropolis ratio is \\[\\begin{align*} a &amp; = \\frac{[\\mathbf{y} | \\theta^{\\star}] [\\theta^{\\star}] } { [\\mathbf{y} | \\theta^{(k-1)}] [\\theta^{(k-1)}] }. \\end{align*}\\] Given an asymmetric proposal \\([\\theta^{\\star}|\\theta^{(k-1)}]\\), the Metropolis-Hastings ratio is \\[\\begin{align*} a &amp; = \\frac{[\\mathbf{y} | \\theta^{\\star}] [\\theta^{\\star}] } { [\\mathbf{y} | \\theta^{(k-1)}] [\\theta^{(k-1)}] } \\frac{[\\theta^{(k-1)} | \\theta^{\\star}]} {[\\theta^{\\star} | \\theta^{(k-1)}]}, \\end{align*}\\] where \\(\\frac{[\\theta^{(k-1)} | \\theta^{\\star}]} {[\\theta^{\\star} | \\theta^{(k-1)}]}\\) is a correction factor that accounts for the asymmetric proposal. The Metropolis algorithm can be shown to be a special case of the Metropolis-Hastings algorithm by noting that \\(\\frac{[\\theta^{(k-1)} | \\theta^{\\star}]} {[\\theta^{\\star} | \\theta^{(k-1)}]} = 1\\) when the proposal is symmetric. 14.4 Example Let’s consider an AR(1) model \\[\\begin{align*} y_t &amp; = \\phi y_{t-1} + \\varepsilon_t \\end{align*}\\] where \\(\\varepsilon_t \\stackrel{iid} \\sim \\operatorname{N}(0, \\sigma^2)\\) and both \\(\\phi\\) and \\(\\sigma^2\\) are unknown. Simulated data are shown below. N &lt;- 500 phi_sim &lt;- 0.99 sigma2_sim &lt;- 1.2 y &lt;- rep(0, N) for (t in 2:N) { y[t] &lt;- phi_sim * y[t-1] + rnorm(1, 0, sqrt(sigma2_sim)) } plot(y, type=&#39;l&#39;, xlab=&quot;time&quot;, main=&quot;simulated AR(1) data&quot;) To complete the model, we must assign prior distributions for \\(\\phi\\) and \\(\\sigma^2\\). A simple choice of prior for \\(\\phi\\) is Uniform(-1, 1) prior and for \\(\\sigma^2\\) we assign a half-Cauchy prior where \\[\\begin{align*} \\sigma^2 &amp; \\sim \\operatorname{Cauchy^+}(s^2) \\end{align*}\\] with \\(s^2\\)=25 where the half-Cauchy has the pdf \\[\\begin{align*} [\\sigma^2] = \\frac{2 * I(\\sigma^2 &gt; 0)}{\\pi s (1 + \\frac{\\sigma^2}{s^2})}. \\end{align*}\\] 14.5 Posterior The posterior distribution is \\[\\begin{align*} [\\phi, \\sigma^2 | \\mathbf{y}] &amp; = \\prod_{t=1}^N [y_t | y_{t-1}, \\phi, \\sigma^2] [\\phi] [\\sigma^2] \\end{align*}\\] 14.6 Conditional posterior for \\(\\phi\\) The conditional posterior for \\(\\phi\\) is \\[\\begin{align*} [\\phi | \\cdot] &amp; \\propto \\prod_{t=1^N} [y_t | y_{t-1}, \\phi, \\sigma^2] [\\phi] \\\\ &amp; \\propto \\prod_{t=1^N} N(y_t | \\phi y_{t-1}, \\sigma^2 ) I(-1 &lt; \\phi &lt; 1) \\end{align*}\\]} which has no known analytic distribution so we will sample using Metropolis-Hastings. 14.7 Conditional posterior for \\(\\sigma^2\\) The conditional posterior for \\(\\phi\\) is \\[\\begin{align*} [\\phi | \\cdot] &amp; \\propto \\prod_{t=1^N} [y_t | y_{t-1}, \\phi, \\sigma^2] [\\sigma^2] \\\\ &amp; \\propto \\prod_{t=1^N} N(y_t | \\phi y_{t-1}, \\sigma^2 ) \\operatorname{Cauchy^+}(\\sigma^2 | s^2) \\end{align*}\\] which has no known analytic distribution so we will sample using Metropolis-Hastings. 14.8 MCMC using a symmetric proposal (Metropolis) library(LaplacesDemon) K &lt;- 5000 phi &lt;- rep(0, N) sigma2 &lt;- rep(0, N) ## initialize phi and sigma2 phi[1] &lt;- runif(1, -1, 1) sigma2[1] &lt;- runif(1, 1, 5) ## set the tuning parameters phi_tune &lt;- 0.0095 accept_phi &lt;- 0 sigma2_tune &lt;- 0.125 accept_sigma2 &lt;- 0 ## ## MCMC loop ## for (k in 2:K) { ## sample phi phi_star &lt;- rnorm(1, phi[k-1], phi_tune) ## check to make sure phi_star is reasonable, if not, we reject and move on if (phi_star &gt; -1 &amp; phi_star &lt; 1) { mh1 &lt;- sum(dnorm(y[2:N], phi_star * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) mh2 &lt;- sum(dnorm(y[2:N], phi[k-1] * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) a &lt;- exp(mh1-mh2) if (runif(1) &lt; a) { ## accept the proposed value phi[k] &lt;- phi_star accept_phi &lt;- accept_phi + 1/K } else { phi[k] &lt;- phi[k-1] } } else { ## phi_star was outside the support, keep at the previous value phi[k] &lt;- phi[k-1] } ## sample sigma2 sigma2_star &lt;- rnorm(1, sigma2[k-1], sigma2_tune) ## check to make sure sigma2_star is reasonable, if not, we reject and move on if (sigma2_star &gt; 0) { mh1 &lt;- sum(dnorm(y[2:N], phi[k] * y[1:(N-1)], sqrt(sigma2_star), log=TRUE)) + dhalfcauchy(sigma2_star, sqrt(25), log=TRUE) mh2 &lt;- sum(dnorm(y[2:N], phi[k] * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) + dhalfcauchy(sigma2[k-1], sqrt(25), log=TRUE) a &lt;- exp(mh1-mh2) if (runif(1) &lt; a) { ## accept the proposed value sigma2[k] &lt;-sigma2_star accept_sigma2 &lt;- accept_sigma2 + 1/K } else { sigma2[k] &lt;- sigma2[k-1] } } else { ## sigma2_star was outside the support, keep at the previous value sigma2[k] &lt;- sigma2[k-1] } } The trace plots showing the parameter estimates, post burn-in, are below. From these, we see that the MCMC model is estimating the parameters reasonably. burnin &lt;- 1000 samples &lt;- cbind(phi[-c(1:burnin)], sigma2[-c(1:burnin)]) colnames(samples) &lt;- c(&quot;phi&quot;, &quot;sigma2&quot;) layout(matrix(1:2, 2, 1)) plot(samples[, &quot;phi&quot;], type=&#39;l&#39;, main=paste0(&quot;Trace plot for phi, acceptance=&quot;, round(accept_phi, digits=2))) abline(h=phi_sim, col=&quot;red&quot;) plot(samples[, &quot;sigma2&quot;], type=&#39;l&#39;, main=paste0(&quot;Trace plot for sigma2, acceptance=&quot;, round(accept_sigma2, digits=2))) abline(h=sigma2_sim, col=&quot;red&quot;) library(coda) effectiveSize(samples) ## phi sigma2 ## 852.8807 875.1672 In the model above, we used symmetric proposals and threw out parameter values that were outside the allowable range. Because \\(\\phi\\) has a true value close to the limit of allowable values, the Metropolis algorithm could be very inefficient. To improve efficiency, let’s rewrite the algorithm using truncated normal proposals (which are asymmetric). library(LaplacesDemon) library(truncnorm) K &lt;- 25000 phi &lt;- rep(0, N) sigma2 &lt;- rep(0, N) ## initialize phi and sigma2 phi[1] &lt;- runif(1, -1, 1) sigma2[1] &lt;- runif(1, 1, 5) ## set the tuning parameters phi_tune &lt;- 0.0175 accept_phi &lt;- 0 sigma2_tune &lt;- 0.175 accept_sigma2 &lt;- 0 ## ## MCMC loop ## for (k in 2:K) { ## sample phi ## use truncated normal proposal phi_star &lt;- rtruncnorm(1, a=-1, b=1, phi[k-1], phi_tune) ## No need to check if phi_star is between -1 and 1 mh1 &lt;- sum(dnorm(y[2:N], phi_star * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) + log(dtruncnorm(phi[k-1], a=-1, b=1, phi_star, phi_tune)) mh2 &lt;- sum(dnorm(y[2:N], phi[k-1] * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) + log(dtruncnorm(phi_star, a=-1, b=1, phi[k-1], phi_tune)) a &lt;- exp(mh1-mh2) if (runif(1) &lt; a) { ## accept the proposed value phi[k] &lt;- phi_star accept_phi &lt;- accept_phi + 1/K } else { phi[k] &lt;- phi[k-1] } ## sample sigma2 sigma2_star &lt;- rtruncnorm(1, a=0, b=Inf, sigma2[k-1], sigma2_tune) ## No need to check to make sure sigma2_star is positive mh1 &lt;- sum(dnorm(y[2:N], phi[k] * y[1:(N-1)], sqrt(sigma2_star), log=TRUE)) + dhalfcauchy(sigma2_star, sqrt(25), log=TRUE) + log(dtruncnorm(sigma2[k-1], a=0, b=Inf, sigma2_star, sigma2_tune)) mh2 &lt;- sum(dnorm(y[2:N], phi[k] * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) + dhalfcauchy(sigma2[k-1], sqrt(25), log=TRUE) + log(dtruncnorm(sigma2_star, a=0, b=Inf, sigma2[k-1], sigma2_tune)) a &lt;- exp(mh1-mh2) if (runif(1) &lt; a) { ## accept the proposed value sigma2[k] &lt;-sigma2_star accept_sigma2 &lt;- accept_sigma2 + 1/K } else { sigma2[k] &lt;- sigma2[k-1] } } The trace plots below show the increased efficiency in the acceptance rate at the same tuning parameter value and show better effective sample size. burnin &lt;- 1000 samples &lt;- cbind(phi[-c(1:burnin)], sigma2[-c(1:burnin)]) colnames(samples) &lt;- c(&quot;phi&quot;, &quot;sigma2&quot;) layout(matrix(1:2, 2, 1)) plot(samples[, &quot;phi&quot;], type=&#39;l&#39;, main=paste0(&quot;Trace plot for phi, acceptance=&quot;, round(accept_phi, digits=2))) abline(h=phi_sim, col=&quot;red&quot;) plot(samples[, &quot;sigma2&quot;], type=&#39;l&#39;, main=paste0(&quot;Trace plot for sigma2, acceptance=&quot;, round(accept_sigma2, digits=2))) abline(h=sigma2_sim, col=&quot;red&quot;) hist(samples[,&quot;phi&quot;], breaks=100) abline(v=phi_sim, col=&quot;red&quot;) abline(v=mean(samples[, &quot;phi&quot;]), col=&quot;blue&quot;) abline(v=quantile(samples[, &quot;phi&quot;], prob=c(0.025, 0.975)), col=&quot;green&quot;, lwd=2) hist(samples[,&quot;sigma2&quot;], breaks=100) abline(v=sigma2_sim, col=&quot;red&quot;) abline(v=mean(samples[, &quot;sigma2&quot;]), col=&quot;blue&quot;) abline(v=quantile(samples[, &quot;sigma2&quot;], prob=c(0.025, 0.975)), col=&quot;green&quot;, lwd=2) library(coda) effectiveSize(samples) ## phi sigma2 ## 5821.186 5192.186 "]
]
